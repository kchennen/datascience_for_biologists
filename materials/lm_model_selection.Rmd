---
title: "Introduction to Model Selection"
author: "Stephanie J. Spielman"
date: "Data Science for Biologists, Spring 2020"
output: 
  html_document:
    css: lm_files/lm_style.css
    highlight: zenburn
    theme: spacelab
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(broom) 
library(patchwork)
```

> CAUTION: This is NOT a full-fledged statistics course. This document focuses on general concepts and only a mild foray into model selection

## Setup

Again, let's use some crabs data:
```{r, collapse=T}
crabs <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/materials/lm_files/crabs.csv")
dplyr::glimpse(crabs)
```




## Introduction to model selection

Sometimes, we have a specific hypothesis we want to test from an experiment or similar: How do <these quantities I have measured> predict <this quantity I am studying>? Other times, and in particular in times of BIG DATA, we have a different scenario: There is lots of data, and it's hard to tell which predictors should and should not be included in a given model. What information will maximize the signal while reducing noise? What is the appropriate level of complexity for a given model? More parameters (aka, including more predictors) lead to increased complexity, but can also be misleading if those predictors hinder more than they help the predictive effort.

### Background: Bias-variance tradeoff
This need leads into two other related key concepts in statistical modeling: a) the tradeoff between bias and variance, and the b) the need to avoid models that *overfit* (or underfit!) the data. The underlying issue is well-exemplified by some figures from [this concise and informative blog post](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229):

![](./lm_files/bias_variance.png){width=300px;}

Ultimately, we want to use our models to predict future outcomes: given certain predictor variable values, what do we expect the response value will be?  In an ideal world, we will craft models with *low variance* (predictions are not spread out with excessive noise) and *low bias* (predictions are systematically close to the truth). We also want to craft models that are *well fit* to the data and do not suffer from *underfitting* (model is insufficiently complex and will not do a good job predicting) or *overfitting* (model is too complex and therefore also won't do a good job predicting). 

![](./lm_files/overfit_underfit.png){width=300px;}

When we build a given model, we give the `lm()` function all our data, and we hope to be able to apply the model to future data we receive. The data used to build a model is called the "training data." As we will learn soon, building a model and saying "hey that's a nice R^2!" is *seriously insufficient* when it comes to evaluating the performance of a model. Instead, what we really want to know is: How accurately does the model predict the response *when applied to data it has never seen before?* To truly evaluate how well a model performs, then, we'd want to supply it with a so-called set of "test data" after training the model. Models that are overfit, in particular, tend to do a very poor job on test data - they are so in tune to the training data, that they can't accomodate the fact that not all the data in the world is in the training data itself.


### Choosing our predictors 

In addition to evaluating model performance (coming up!), we desire a strategy to choose our predictors wisely: Find which predictors add *meaningful information* and do not add excessive *too much noise* that would lead to overfitting. These procedures are known as "model selection": of the set of models we could build (i.e., of all the predictors we could include), which model should we build (i.e., which predictors should be used) that give us the best chance for predictions that are low variance and low bias? There are a wide variety of approaches to this task, and we will demonstrate one common approach called "stepwise model selection." In stepwise model selection, the computer will systematically identify one-by-one which predictors are not "helpful," according to some measurement, and proceed to build the model using only those helpful predictors. It can proceed forwards (start with one predictor and ramp up), or backwards (start with all predictors and ramp down).

One way to do this is to systematically remove, one by one, any *insignificant* predictors, stopping when only significant predictors remain. Another way to do this is to check how each predictor influences the model's $R^2$ value - any predictors that worsen $R^2$ should get booted because they lead to less explanatory power. Finally, there are other more advanced metrics known as"information theoretic criterion" (Akaike Information Criterion [AIC] and similar) which can be used in lieu of $R^2$. These measures ultimately penalize models with too many low-information predictors to help distill the model to only those predictors which increase predictive power while ensuring the overall model is not overly complex. These scores are like "golf" - a lower AIC indicates a model with improved *fit to the data*. They should also be interpreted *relative* to each other, unlike a P-value whose exact value can be directly understood. 

#### AIC stepwise model selection

There are many R libraries to perform stepwise model selection, but to simplify our lives we will simply use the base R function `step()`, which uses AIC-based regression model selection. We can perform model selection by building a model with *all possible predictors*, and then letting `step()` figure out the right set to use. In fact, this is luckily very easy! Let's do this procedure to get the best model for predicting sparrow body_depth:

```{r}
# comprehensive model: specify "all other variables" with a period 
baseline_model_fit <- lm(body_depth ~ ., data = crabs)

# add the argument trace=F to hide the stepwise output
# here, we keep default trace=T to see what's happening
final_model_fit <- step(baseline_model_fit) 
```


In the end, we see that the best model (using AIC to select predictors) is saved into `final_model_fit`:
```{r}
summary(final_model_fit)
```

Importantly, this procedure *does not consider interaction effects* - for our purposes, that's ok! It also has the tendancy to sometimes keep predictors that are not significant, i.e. frontal lobe in this scenario. In fact, these are kept because, according to AIC, these predictors do not *detract* from our predictive ability, so they do not need to be removed. 

**In the end**, our final model for body depth has three predictors (color, frontal lobe, and carapace length), and together these predictors can explain 98.81% of the variation in sparrow body_depth, highly significant at *P<2.2e-16*. (PS: You'll notice we did lots of models with color and carapace length in this document? I skipped ahead and found the best predictors so as to demonstrate these concepts to you - no coincidence here!)

#### Culling insignificant predictors model selection

Let's do this one other, more tedious way: We'll remove insignificant predictors one at a time (highest P-value gets chucked at each round), until only significant predictors remain. Using `broom::tidy()` makes this experience a lot easier!!
```{r, collapse=T}
# start with all predictors
lm(body_depth ~ ., data = crabs) %>%
  broom::tidy()

# carapace_width was most insignificant. remove and continue
crabs %>% select(-carapace_width) -> sub_crabs
lm(body_depth ~ ., data = sub_crabs) %>%
  broom::tidy()

# rear_width was most insignificant. remove and continue
sub_crabs %>% select(-rear_width) -> sub_crabs
lm(body_depth ~ ., data = sub_crabs) %>%
  broom::tidy()

# sex was most insignificant. remove and continue
sub_crabs %>% select(-sex) -> sub_crabs
lm(body_depth ~ ., data = sub_crabs) %>%
  broom::tidy()

# frontal_lobe is now the only insignificant. remove and continue
sub_crabs %>% select(-frontal_lobe) -> sub_crabs
lm(body_depth ~ ., data = sub_crabs) %>%
  broom::tidy()

### STOP!!! All significant remain!!!
final_fit <- lm(body_depth ~ ., data = sub_crabs)
summary(final_fit)
```

In the end, we got to the same place as with AIC (but there is no guarantee any two model selection methods will agree!!!), except we removed frontal lobe. You'll note, though, the final $R^2=0.988$ with or without frontal lobe - AIC was "right" that it doesn't hurt to keep frontal lobe in the model.

