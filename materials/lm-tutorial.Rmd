---
title: "Introduction to Linear Modeling: Part 1"
author: "Stephanie J. Spielman"
date: "3/25/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(broom) 
```

> CAUTION: This is NOT a full-fledged statistics course. This document focuses on performing and interpretting linear models rather than their mathematical underpinnings.

<!--
## Resources

+ ["Statistical Inference via Data Science" Chapters 5-6](https://bookdown.org/cteplovs/ismaykim/6-regression.html)
-->


## What is a linear model?

In statistics and data science, we often want to describe how the value of one variable *depends on and/or can be predicted by* the value of one or more other variables. For example, if we know an individual's height, could we reasonably predict their weight? Potentially - we might want to also consider a lot more information like the person's age, biological sex, health and diet, etc, in addition to just their height. The variable we are interested in predicting (here, *weight*) is known as the **response or dependent variable**. We are interested in seeing how certain other variables can provide meaningful information about weight, and we call these other variables **predictor or independent variables**.

The term "linear model" implies a statistical framework for quantifying to what degree *one or more predictor variables* describes the variation seen in a *response variable*. Linear models can answer questions like...
+ Which of the predictor variables show a significant relationship with the response variable?
  + In this case, *significant* (more or less..) implies that the predictor variable's values influence, to some degree, values of the response variable. An *insignificant* (again, more or less..) predictor is one whose influence on the response is no different from random chance.
+ How does the response value change when the predictor value changes?
+ How much variation in the response variable does each predictor variable explain?


As described above, linear models can only model **numeric response variables.** There are other types of closely-related statistical models that one can use for modeling categorical response variables. For example, the pr


The mathematical formula, generally speaking, for the simplest linear model:

\begin{equation} 
  Y = \beta_1X_1 + \beta_0 + \epsilon
\end{equation} 


The $Y$ is our response, and all $X$ variables are our predictors - in the simple example above, there is a single predictor $X_1$. Each $\beta$ (Greek letter "beta") is a given predictor's *coefficient*, and they quantify the relationship between each predictor and the response. The $\epsilon$ (Greek letter "epsilon") represents the *error* in the model, or in other words, what percent of variation in the response variable can NOT be explained by the predictor variable(s)? In fact, the formula above is actually the formula for a line, or as you may be used to seeing it, $Y= mX + B$;  the $\beta_1$ is the slope $m$, and the $\beta_0$ is the intercept $B$. 

More generally for $N$ predictors, we can write the formula as (rearranged slightly)

\begin{equation} \label{eq:full}
  Y =  \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 ... + \beta_NX_N + \epsilon
\end{equation} 


The error term $\epsilon$ is known as the model's *residuals* - how many variation in the response is residual (left over) after considering all the predictors? It is virtually never true that the predictors will capture 100% of the variation in the response, and the uncaptured percent falls into that $\epsilon$ term. Practically, what does this mean? We can visualize this most easily with a simple linear regression where a *line of best fit* is **FIT** (get it? line of best FIT?) to the data. This line of best fit IS the linear model - we will call it the regression line. 

One of the most common procedures for fitting a model is called ["Ordinary Least Squares,"](https://en.wikipedia.org/wiki/Ordinary_least_squares) which is an algorithm that finds the line which minimizes the "sum of squares" of the residuals. The residuals themselves are the *distance* between each point and the regression line, and the sum of squares in this case is the summed of the residuals sqursquared distances of all data points to regression line (loosely). 

**Consider this example** (images from [this book](https://www.amazon.com/Analysis-Biological-Data-Michael-Whitlock/dp/1936221489)) that shows the relationship between the age of a given lion the the proportion of its nose that is black (nose color changes over time for cats and dogs! awwww):

![](./lm_files/whitlock_17.1-1.png){width=200px}

A linear model for this data would be the simple regression, $Y = \beta_1X_1 + \beta_0$ aka a line.
The computer (hooray!) will try to find the best combination of slope ($\beta_1$) and Y-intercept ($\beta_0$) that makes the residual sum of squares as small as possible by testing out hundreds or thousands of different values. This is what we mean by **fitting a model**: What set of *parameter* (fancy word for variables) match the data the best?

In the three images below, we see three different ways this regression line could be drawn. Each line segment from the the point to the regression line is a residual!

![](./lm_files/whitlock_17.1-2.png){width=500px}

+ "Large deviations": Very high sum of squares. This line is a poor fit to the data.
+ "Smaller deviations": A low sum of squares. The line is a better, but not best, fit to the data.
+ "Smallest deviations": This line represents the line that minimizes the residuals (which the computer found after trying hundreds or thousands of options!. Its corresponding slope and intercept are our *model parameters*. It turns out this slope is about $\beta_1 = 10.64$ and the intercept is about $\beta_0 = 0.88$. Our final fitted model would therefore be $Y = 10.64X_1 + 0.88 + \epsilon$. In the coming sections we will learn how to interpret these quantities.


### Assumptions

In this class, we will talk about two general (NOTE: this is a pun, because actually we are talking about "generalized linear models". You should be aware that my language choice is actually really funny) types of linear models:

+ "Linear regression", which are used to model **NUMERIC response variables** (this document)
+ "Logistic regression", which are used to model **BINARY (categorical) response variables** (e.g., "Yes/No" categories) (a forthcoming document)

These models make several key assumptions about the underlying predictors and response variable. These assumptions represent properties in the data itself which must be satisified in order for the statistics to be reliably interpreted.

+ Linear regression models assume:
  + *Numeric predictors* have a linear relationship with the numeric response. For example, in the two plots below, the LEFT shows a linear relationship: This data is suitable for analysis with a linear regression The RIGHT shows a non-linear relationship: This data is not suitable for analysis with a regression (unless you transform\*) the data. 
```{r, echo=F, warning = F, message=F, fig.width = 8, fig.height=4}
tibble(y = 10:60) %>%
  mutate(x1 = y * runif(51, 1.5, 2), x2 = y**3) -> dat
ggplot(dat, aes(x = x1, y=y))  +geom_point() + geom_smooth(method = "lm", se =F) + ggtitle("Reasonably linearly related") + xlab("X") + ylab("Y")-> p1
ggplot(dat, aes(x = x2, y=y))  +geom_point() + geom_smooth(method = "lm", se =F) + ggtitle("Clearly NOT linearly related")+ xlab("X") + ylab("Y")-> p2
p1 + p2
```
  + *Categorical predictors* should have uniformly-distributed variance. **TODO**

  + The *residuals* of the model should be normally-distributed (a bell curve). There is another type of plot we will see later in this document for more carefully examining residuals.
```{r, echo=F, warning = F, message=F, fig.width = 8, fig.height=4}
tibble(x = rnorm(500)) %>%
  ggplot(aes(x = x)) +geom_density(fill = "cadetblue") + xlab("residuals") + ggtitle("Reasonably normally distributed") -> p1
tibble(x = rexp(500)) %>%
  ggplot(aes(x = x)) +geom_density(fill = "darkorchid3") + xlab("residuals") + ggtitle("Clearly NOT normally distributed")-> p2
p1 + p2
```
  + Your numeric data itelf should also be normally distributed, but thanks to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) we can largely "ignore" this assumption for most circumstances we'll see (aka more than 100 or so data points.)

### Analyses that are actually all linear regression models

You will often hear these fancy statistical names for different tests. Fundamentally, they are all linear regressions with different types of predictors. Really there is no need to distinguish them!! You just need to know how to interpret predictor coefficients, and you can make any model you want. Again, all linear regressions have a **numeric response**. 

+ Simple linear regression: Models with a single *numeric* predictor variable
+ Multiple linear regression: Models with several  *numeric* predictor variable
+ ANOVA (**An**alysis **o**f **Va**riance): Models with a single *categorical* predictor variable
+ MANOVA (**M**ultivariate **An**alysis **o**f **Va**riance): Models with a multiple *categorical* predictor variables
+ ANCOVA (**An**alysis of **Cova**riance): Models with a single *categorical* AND one or more *numeric* predictor variables
+ MANCOVA (**M**ultivariate **An**alysis of **Cova**riance): Models with multiple *categorical* AND multiple *numeric* predictor variables





## Examples and interpretation



#### This code chunk is your function cheatsheet
```{r, eval=F, error=T}
# Single predictor
lm(Y ~ X, data = <dataframe>) -> lm_output

# Multiple independent predictors
lm(Y ~ X + X1 + X2, data = <dataframe>) -> lm_output

# Multiple predictors with an interaction effect
lm(Y ~ X*X1, data = <dataframe>) -> lm_output

broom::tidy(lm_output)   ## See output of model but tidy!
broom::glance(lm_output) ## See other output of model but tidy!
broom::augment(lm_output) ## Get mmmooorrree information
```

> All examples will use the external dataset `sparrows`

```{r}
sparrows <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/materials/lm_files/sparrows.csv")
dplyr::glimpse(sparrows)
```

### Simple linear regression: Single numeric predictor

Let's begin with a simple regression to examine the relationship between **Weight** and **Skull Length**. We wish to know whether weight can be predicted from skull length. Therefore, skull lengthis our predictor (X) variable, and weight is our response (Y) variable.


**The very first thing we should do is VISUALIZE our data** (when there are $\leq2$ predictors, it's doable). Get an initial sense of what to expect, and *and* see if data are indeed linearly related. Indeed, this relationship looks fairly linear, and apparantly positively related, so we are good to go.

```{r, fig.width = 3, fig.height = 3}
# predictor goes on X, response goes on Y
ggplot(sparrows, aes(x = Skull_Length, y = Weight)) + 
  geom_point()
```



We perform regressions using the function `lm()` (any guesses what this stands for?).

```{r}
## perform linear regression and save as model_fit
model_fit <- lm(Weight ~ Skull_Length, data = sparrows)

## view output with summary(). Ugly, useful for getting a visual overview.
summary(model_fit)
```


Let's go line-by-line through the output:

+ **Call** simply reminds us of the linear model formula we specified when using `lm()`
+ **Residuals** shows the *five-number summary* of the residuals (in this case, distance of each point to the line of best-fit).
+ **Coefficients** tell us our $\beta$ values. Their *Estimate* is the value the model returned, Their *Std. Error* indicates how close/far these model estimates, derived from the sample, likely are from the "true" population values. The *t value* is *t* statistic associated with the estimate's significance, and the final column *Pr(>|t|)  * is the P-value associated with the *t* statistic - it tells us if the coefficient is significant or not.
  + **Intercept**: This is our $\beta_0$ and it tells us: **What is the EXPECTED VALUE of the response variable when the numeric predictor value is 0?** In this case, what do we expect the Weight will be when Skull Length is 0? Here, we expect -8.149  - this is of course NOT realistic and illustrates perfectly that you need to be careful to INTERPRET COEFFICIENTS WITH BIOLOGICAL INTUITION!! Just because the computer spits out a number doesn't mean it's meaningful. No sparrow will have a skull length of 0.
    + **Intercept significance**: The P-value for the intercept is derived from a standard null hypotheis: "The intercept = 0." Significant P-values can be interpreted to mean that it is unlikely the true intercept is 0 (but caution, this is NOT exactly what P-values mean). In this case, the intercept is actually NOT significant (P=0.0998) - therefore, we can only conclude that "there is no evidence the intercept differs from 0."
  + **Skull_Length**: This is our $\beta_1$ and it tells us: **What is the EXPECTED change of the response the predictor increases by a unit of 1?** In this case, by how much do we expect Skull Width will increase/decrease when the Skull Length goes up by 1? Here, we expect for every unit change in skull length, skull width *increases* by 1.0663. Remember in this case, this value is the slope!!!
    + **Coefficient significance**: The P-value for any numeric intercept is derived from a standard null hypotheis: "The coefficient = 0." Significant P-values can be interpreted to mean that it is unlikely the true coefficient is 0 (but caution, this is NOT exactly what P-values mean). The P-value here is highly significant ($P=2.45e-10$), so we are reasonably confident that the true value indeed differs from 0.
+ **Residual standard error**: The standard error associated with residuals. Don't worry much about this line for thsi class.
+ **Multiple R-squared** and	**Adjusted R-squared** give the **R^2** associated with the model!! This is SUPER important. The "adjusted R^2" is somewhat more reliable for consideration - it corrects for any overfitting artifacts inherent in modeling, so focus on this value. **A model's R^2 tells you the percent of variation in the response that can be explained by the predictors**. It's associated P-value is on the next line (here, `p-value: 2.446e-10`), and the associated standard null for this P-value is "R^2 = 0."
  + **In this model, 25.388% of the variation in skull widths can be explained by skull lengths. 74.62% of the varation in skull widths is therefore UNEXPLAINED by our model.** That 74.62% is sneakily hidden in the $\epsilon$ term of the linear model equation. 
  
  
**From this model, we have discovered the following:**

+ ~25.3% of variation in sparrow skull widths can be explained by skull lengths. While highly significant, this is a relatively low $R^2$ (perhaps we would have benefitted by including more information in our model!). Remember: effect size is NOT the same as significance!
+ For every unit increase in skull length, we expect weight to increase, on average, by ~1.0663.
+ We did not reject the null that the intercept is 0, so we expect expect the average sparrow with a skull length of 0 to have a weight of 0. But we also recognize that this is ridiculous and just because the math says it doesn't mean it's biologically meaningful.


### Simple ANOVA: Single categorical predictor

What if, rather than a numeric predictor (Skull Length), we had a *categorical* predictor, say Age? Here we might ask: **Does age predict weight in sparrows**? 

Again, begin with a quick-and-dirty visualization. Here we'd like to see that the distribution of skull widths has similar *spread* between ages (assumption of equal variance for categorical predictors!). Indeed, these two distributions show similar amounts of spread (how much relative space along the Y axis they take up), so assumption is met.

```{r, fig.width = 3, fig.height = 3}
ggplot(sparrows, aes(x = Age, y = Weight, color=Age)) + geom_jitter()
```


Let's run the model:
```{r}
## perform linear model and save as model_fit
model_fit <- lm(Weight ~ Age, data = sparrows)

## view output with summary(). 
summary(model_fit)
```

*This is where factors become really important*: All linear model output when categorical predictors are used assumes a given LEVEL of the categorical variable. Unless you re-factor a variable explicitly, levels will be used in alphabetical order - here, "Adult" comes before "Young".

We see the intercept is highly significant ($P<2e-16$) with a value of 25.49. For a model with a categorical predictor, this value means: *What is the expected weight for your average ADULT (the first level of Age) sparrow*? 

The coefficient here is associated with `AgeYoung` - it means, on average, we expect YOUNG birds to have weights on average 0.0765 units larger compared to Adult birds. Notably, for a categorical variable with $N$ levels, there will be see $N-1$ coefficients (one for each of the non-default). *Interpret categorical coefficients as the relative increase (or decrease if negative!) of response for this predictor level compared to the first/default predictor level.* The default here is Adult (alphabet!!). In this case, however, the P-value is NOT significant (P=0.772). Similarly our adjusted $R^2$ is not significant.

**This model therefore tells us: Age is NOT predictive of weight in sparrows!**
 

### LM with numeric and categorical predictors

Let's see how to run and interpret a model with BOTH numeric and categorical predictors. We will examine how Age AND Skull Length might be predictive of Weight in sparrows. Importantly, when you have multiple predictors, the model assumes they are FULLY INDEPENDENT and ENTIRELT UNRELATED. Of course this is not always true, and the next section shows how to deal with that issue.

Let's run the model with multiple predictors, which we simply add together in the formula
```{r}
## perform linear model and save as model_fit
model_fit <- lm(Weight ~ Age + Skull_Length, data = sparrows)

## view output with summary(). 
summary(model_fit)
```

When considering BOTH Age and Skull Length, we find:

+ The intercept means your average Adult bird with a skull length of 0 has a weight of -8.26. It's insigificant and unrealistic - moving on.
+ The "AgeYoung" means *when controlling for skull length*, each Young bird will weigh ~0.118 more than an Adult bird. Again, it's insigificant so we actually have no evidence the weight increase between Adult/Young differs from 0.
+ The "Skull_Length" coefficient means *when controlling for age*, the average bird's weight increases by 1.07 for each unit increase in skull length, and it's highly significant! Something! 
+ The predictors explain $~24.9%$ of the variation in weight. **Critically, this is LESS THAN the model with Skull_Length as the only predictor** (before, $R^=0.2538$). By including the Age predictor, we in fact added NOISE rather than INFORMATION to the model - so we are actually LESS predictive.
+ ~75% of variation in weight is unexplained by the significant predictor skull_length.


### LM with multiple numeric predictors

Now we will look at a model with multiple numeric predictors, Skull Width *and* Skull Length. We know already that skull length has a linear relationship with weight from earlier in this document, so let's just check out the linearity of weight/skull width. Indeed, the relationship is linear, so let's proceed.

```{r, fig.width = 3, fig.height = 3}
ggplot(sparrows, aes(x = Skull_Length, y = Weight)) + geom_point()
```

 summary(lm(Weight ~ Skull_Width + Skull_Length, data = sparrows))

### LM with interaction effects


 summary(lm(Weight ~ Femur_Length * Skull_Length, data = sparrows))

 summary(lm(Weight ~ Femur_Length * Skull_Length, data = sparrows))

The problem with the last two analyses was that they ASSUMED the predictors were fully independent of one another - this assumption of independent predictors is in fact a key assumption of linear models. But of course, it is not always the case that predictors are completely independent - Age and Skull Length, and almost certainly Skull Length and Skull Width, themselves are probably related to a degree! We can directly test and/or control for this non-independence using **interaction effects**. 

The rule is **significant interacton effects trump significant independent effects.**

width length




















### When would you need to transform??

Below are some examples of when data transformation is needed:

1. In the `diamonds` dataset price and carat should be logged
2. In the `msleep`dataset weight needs to be logged


```{r}
## view output with broom::tidy() and broom::glance(). Pretty, useful for tibble-oriented data analysis
broom::tidy(model_fit)
broom::glance(model_fit)
```

