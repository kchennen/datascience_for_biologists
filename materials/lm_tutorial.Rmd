---
title: "Introduction to Linear Modeling"
author: "Stephanie J. Spielman"
date: "Data Science for Biologists, Spring 2020"
output: 
  html_document:
    theme: spacelab
    highlight: zenburn
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(broom) 
```

> CAUTION: This is NOT a full-fledged statistics course. This document focuses on performing and interpretting linear models rather than their mathematical underpinnings.

<!--
## Resources

+ ["Statistical Inference via Data Science" Chapters 5-6](https://bookdown.org/cteplovs/ismaykim/6-regression.html)
-->


## What is a linear model?

In statistics and data science, we often want to describe how the value of one variable *depends on and/or can be predicted by* the value of one or more other variables. For example, if we know an individual's height, could we reasonably predict their weight? Potentially - we might want to also consider a lot more information like the person's age, biological sex, health and diet, etc, in addition to just their height. The variable we are interested in predicting (here, *weight*) is known as the **response or dependent variable**. We are interested in seeing how certain other variables can provide meaningful information about weight, and we call these other variables **predictor or independent variables**.

The term "linear model" implies a statistical framework for quantifying to what degree *one or more predictor variables* describes the variation seen in a *response variable*. Linear models can answer questions like...
+ Which of the predictor variables show a significant relationship with the response variable?
  + In this case, *significant* (more or less..) implies that the predictor variable's values influence, to some degree, values of the response variable. An *insignificant* (again, more or less..) predictor is one whose influence on the response is no different from random chance.
+ How does the response value change when the predictor value changes?
+ How much variation in the response variable does each predictor variable explain?


As described above, linear models can only model **numeric response variables.** There are other types of closely-related statistical models that one can use for modeling categorical response variables. For example, the pr


The mathematical formula, generally speaking, for the simplest linear model:

\begin{equation} 
  Y = \beta_1X_1 + \beta_0 + \epsilon
\end{equation} 


The $Y$ is our response, and all $X$ variables are our predictors - in the simple example above, there is a single predictor $X_1$. Each $\beta$ (Greek letter "beta") is a given predictor's *coefficient*, and they quantify the relationship between each predictor and the response. The $\epsilon$ (Greek letter "epsilon") represents the *error* in the model, or in other words, what percent of variation in the response variable can NOT be explained by the predictor variable(s)? In fact, the formula above is actually the formula for a line, or as you may be used to seeing it, $Y= mX + B$;  the $\beta_1$ is the slope $m$, and the $\beta_0$ is the intercept $B$. 

More generally for $N$ predictors, we can write the formula as (rearranged slightly)

\begin{equation} \label{eq:full}
  Y =  \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 ... + \beta_NX_N + \epsilon
\end{equation} 


The error term $\epsilon$ is known as the model's *residuals* - how many variation in the response is residual (left over) after considering all the predictors? It is virtually never true that the predictors will capture 100% of the variation in the response, and the uncaptured percent falls into that $\epsilon$ term. Practically, what does this mean? We can visualize this most easily with a simple linear regression where a *line of best fit* is **FIT** (get it? line of best FIT?) to the data. This line of best fit IS the linear model - we will call it the regression line. 

One of the most common procedures for fitting a model is called ["Ordinary Least Squares,"](https://en.wikipedia.org/wiki/Ordinary_least_squares) which is an algorithm that finds the line which minimizes the "sum of squares" of the residuals. The residuals themselves are the *distance* between each point and the regression line, and the sum of squares in this case is the summed of the residuals sqursquared distances of all data points to regression line (loosely). 

**Consider this example** (images from [this book](https://www.amazon.com/Analysis-Biological-Data-Michael-Whitlock/dp/1936221489)) that shows the relationship between the age of a given lion the the proportion of its nose that is black (nose color changes over time for cats and dogs! awwww):

![](./lm_files/whitlock_17.1-1.png){width=250px}

A linear model for this data would be the simple regression, $Y = \beta_1X_1 + \beta_0$ aka a line.
The computer (hooray!) will try to find the best combination of slope ($\beta_1$) and Y-intercept ($\beta_0$) that makes the residual sum of squares as small as possible by testing out hundreds or thousands of different values. This is what we mean by **fitting a model**: What set of *parameter* (fancy word for variables) match the data the best?

In the three images below, we see three different ways this regression line could be drawn. Each line segment from the the point to the regression line is a residual!

![](./lm_files/whitlock_17.1-2.png){width=600px}

+ "Large deviations": Very high sum of squares. This line is a poor fit to the data.
+ "Smaller deviations": A low sum of squares. The line is a better, but not best, fit to the data.
+ "Smallest deviations": This line represents the line that minimizes the residuals (which the computer found after trying hundreds or thousands of options!. Its corresponding slope and intercept are our *model parameters*. It turns out this slope is about $\beta_1 = 10.64$ and the intercept is about $\beta_0 = 0.88$. Our final fitted model would therefore be $Y = 10.64X_1 + 0.88 + \epsilon$. In the coming sections we will learn how to interpret these quantities.


### Assumptions

In this class, we will talk about two general (NOTE: this is a pun, because actually we are talking about "generalized linear models". You should be aware that my language choice is actually really funny) types of linear models:

+ "Linear regression", which are used to model **NUMERIC response variables** (this document)
+ "Logistic regression", which are used to model **BINARY (categorical) response variables** (e.g., "Yes/No" categories) (a forthcoming document)

These models make several key assumptions about the underlying predictors and response variable. These assumptions represent properties in the data itself which must be satisified in order for the statistics to be reliably interpreted.

+ Linear regression models assume:
  + *Numeric predictors* have a linear relationship with the numeric response. For example, in the two plots below, the LEFT shows a linear relationship: This data is suitable for analysis with a linear regression. The RIGHT shows a non-linear relationship: This data is not suitable for analysis with a regression (unless you transform) the data. 
```{r, echo=F, warning = F, message=F, fig.width = 6, fig.height=2}
tibble(y = 10:60) %>%
  mutate(x1 = y * runif(51, 0.5, 2.5), x2 = y**3) -> dat
ggplot(dat, aes(x = x1, y=y))  +geom_point() + geom_smooth(method = "lm", se =F, size=0.5) + ggtitle("Reasonably linearly related") + xlab("X") + ylab("Y")-> p1
ggplot(dat, aes(x = x2, y=y))  +geom_point() + geom_smooth(method = "lm", se =F, size=0.5) + ggtitle("Clearly NOT linearly related")+ xlab("X") + ylab("Y")-> p2
p1 + p2
```
  + *Categorical predictors* should have uniformly-distributed variance. For example, in the two plots below, the LEFT shows a numeric variable whose different distributions *across the categorical variable have about the same spread* (although the means differ!): This data is suitable for analysis with a linear regression. The RIGHT shows an example where variance is NOT equally distributed, and this may not be suitable for analysis with a linear regression.
```{r, echo=F, warning = F, message=F, fig.width = 6, fig.height=2}
tibble("categorical predictor" = c(rep("category 1", 50), rep("category 2", 50), rep("category 3", 50)),
       "response values" = c(rnorm(50, 10, 2), rnorm(50, 20, 2), rnorm(50, 5, 2))) %>%
  ggplot(aes(x = `categorical predictor`, y = `response values`, color = `categorical predictor`)) +  geom_jitter(size=0.5) + theme(legend.position = "none") + ggtitle("Equal variance across groups :)") -> p1


tibble("categorical predictor" = c(rep("category 1", 50), rep("category 2", 50), rep("category 3", 50)),
       "response values" = c(rnorm(50, 10, 2), rnorm(50, 20, 7), rnorm(50, 5, 0.2))) %>%
  ggplot(aes(x = `categorical predictor`, y = `response values`, color = `categorical predictor`)) +  geom_jitter(size=0.5) + theme(legend.position = "none") + ggtitle("UNEQUAL variance across groups :(") -> p2

p1 + p2
```
  

  + The *residuals* of the model should be normally-distributed (a bell curve). **There is another type of plot, called a "Q-Q plot" ("quantile-quantile") we will see later in this document for more carefully examining residuals.**
```{r, echo=F, warning = F, message=F, fig.width = 6, fig.height=2}
tibble(x = rnorm(500)) %>%
  ggplot(aes(x = x)) +geom_density(fill = "cadetblue") + xlab("residuals") + ggtitle("Reasonably normal") -> p1
tibble(x = rexp(500)) %>%
  ggplot(aes(x = x)) +geom_density(fill = "darkorchid3") + xlab("residuals") + ggtitle("NOT normal")-> p2
p1 + p2
```
  + Your numeric data itelf should also be normally distributed, but thanks to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) we can largely "ignore" this assumption for most circumstances we'll see (aka more than 100 or so data points.)

### Analyses that are actually all linear regression models

You will often hear these fancy statistical names for different tests. Fundamentally, they are all linear regressions with different types of predictors. Really there is no need to distinguish them!! You just need to know how to interpret predictor coefficients, and you can make any model you want. Again, all linear regressions have a **numeric response**. 

+ Simple linear regression: Models with a single *numeric* predictor variable
+ Multiple linear regression: Models with several  *numeric* predictor variable
+ ANOVA (**An**alysis **o**f **Va**riance): Models with a single *categorical* predictor variable
+ MANOVA (**M**ultivariate **An**alysis **o**f **Va**riance): Models with a multiple *categorical* predictor variables
+ ANCOVA (**An**alysis of **Cova**riance): Models with a single *categorical* AND one or more *numeric* predictor variables
+ MANCOVA (**M**ultivariate **An**alysis of **Cova**riance): Models with multiple *categorical* AND multiple *numeric* predictor variables


### Let's not forget correlation!

A closely related (both conceptually and mathematically) topic here is *correlation*, which is a quantity that tells us whether two variables appear to be associated, or non-independent. **Correlation does NOT IMPLY causation** - it merely implies an observable pattern of association. There are many different wants to quantify a correlation, but the most commonly-used one is Pearson correlation. This quantity measures strength and direction a of **linear** association between normally-distributed numeric variables. This is measured with the correlation coefficient *r* which can be any value in $-1 \leq r \leq 1$. Below are shown examples of data with PERFECT correlations: there is a perfect x/y association, but the directions are different. There is also an example of NO correlation.

```{r, echo=F, fig.width=6.5,fig.height=2}
tibble(x = seq(1, 100, 5), y = x*-1, y2 = runif(20, -10, 10)) -> dat

p1 <- ggplot(dat, aes(x=x,y=x)) + geom_point() + geom_abline(color="red", size=0.5) + xlab("Predictor") + ylab("Response") + ggtitle("r = 1")

p2 <- ggplot(dat, aes(x=x,y=y)) + geom_point() + geom_abline(slope = -1, color="red",size=0.5) + xlab("Predictor") + ylab("Response") + ggtitle("r = -1")

p3 <- ggplot(dat, aes(x=x, y=y2)) + geom_point() + geom_hline(yintercept=0, color="red", size=0.5) + xlab("Predictor") + ylab("Response") + ggtitle("r = 0")

p1 + p2 + p3
```


The strength of the relationship is heavily influenced by *noise* (images from [this book](https://www.amazon.com/Analysis-Biological-Data-Michael-Whitlock/dp/1936221489)):

![](./lm_files/whitlock_16.6-1.png){width=600px}


**But be careful!!** The computer will still calculate a correlation even if the data is not linearly related! The correlatio below is NOT VALID because the assumption of linearity has not been met. Always plot your data!!!

```{r,echo=F, fig.width=3, fig.height=2.5}
tibble(x = (1:200)/10, y = exp(x)) -> dat
ggplot(dat, aes(x=x,y=y)) + geom_point(size=0.25) + ggtitle("r = 0.52, but NOT VALID")
```

There are other types of correlations that can be employed (for example, Spearman rank correlation) when the data is not linearly related.


## Examples and interpretation



#### This code chunk is your function cheatsheet
```{r, eval=F, error=T}
# Single predictor
lm(Y ~ X, data = <dataframe>) -> lm_output

# Multiple independent predictors
lm(Y ~ X + X1 + X2, data = <dataframe>) -> lm_output

# Multiple predictors with an interaction effect
lm(Y ~ X*X1, data = <dataframe>) -> lm_output

# Standard view of fitted model
summary(lm_output) 

# Tidy views of fitted model
broom::tidy(lm_output)   ## Coefficients
broom::glance(lm_output) ## R^2 and other _model fit_ metrics
broom::augment(lm_output) ## Get mmmooorrree information
```

> All examples will use the external dataset `sparrows`

```{r}
sparrows <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/materials/lm_files/sparrows.csv")
dplyr::glimpse(sparrows)
```

### Simple linear regression: Single numeric predictor

Let's begin with a simple regression to examine the relationship between **Weight** and **Skull Length**. We wish to know whether weight can be predicted from skull length. Therefore, skull lengthis our predictor (X) variable, and weight is our response (Y) variable.


**The very first thing we should do is VISUALIZE our data** (when there are $\leq2$ predictors, it's doable). Get an initial sense of what to expect, and *and* see if data are indeed linearly related. Indeed, this relationship looks fairly linear, and apparantly positively related, so we are good to go.

```{r, fig.width = 4, fig.height = 3}
# predictor goes on X, response goes on Y
ggplot(sparrows, aes(x = Skull_Length, y = Weight)) + 
  geom_point()
```



We perform regressions using the function `lm()` (any guesses what this stands for?).

```{r}
## perform linear regression and save as model_fit
## Y ~ X !!!!!!!!!!!!!! 
## RESPONSE ~ PREDICTOR !!!!!!!!!!!!!!!!1
model_fit <- lm(Weight ~ Skull_Length, data = sparrows)

## view output with summary(). Ugly, useful for getting a visual overview.
summary(model_fit)
```


#### Interpreting the model
Let's go line-by-line through the output:

+ **Call** simply reminds us of the linear model formula we specified when using `lm()`
+ **Residuals** shows the *five-number summary* of the residuals (in this case, distance of each point to the line of best-fit).
+ **Coefficients** tell us our $\beta$ values. Their *Estimate* is the value the model returned, Their *Std. Error* indicates how close/far these model estimates, derived from the sample, likely are from the "true" population values. The *t value* is *t* statistic associated with the estimate's significance, and the final column *Pr(>|t|)  * is the P-value associated with the *t* statistic - it tells us if the coefficient is significant or not.
  + **Intercept**: This is our $\beta_0$ and it tells us: **What is the EXPECTED VALUE of the response variable when the numeric predictor value is 0?** In this case, what do we expect the Weight will be when Skull Length is 0? Here, we expect -8.149  - this is of course NOT realistic and illustrates perfectly that you need to be careful to INTERPRET COEFFICIENTS WITH BIOLOGICAL INTUITION!! Just because the computer spits out a number doesn't mean it's meaningful. No sparrow will have a skull length of 0. We can also write this as $-8.149\pm4.9168$ to show the standard error! [HINT: to write "plus/minus", you need to write `$\pm$` - dollar sign, \pm, dollar sign.]
    + **Intercept significance**: The P-value for the intercept is derived from a standard null hypotheis: "The intercept = 0." Significant P-values can be interpreted to mean that it is unlikely the true intercept is 0 (but caution, this is NOT exactly what P-values mean). In this case, the intercept is actually NOT significant (P=0.0998) - therefore, we can only conclude that "there is no evidence the intercept differs from 0."
  + **Skull_Length**: This is our $\beta_1$ and it tells us: **What is the EXPECTED change of the response the predictor increases by a unit of 1?** In this case, by how much do we expect Skull Width will increase/decrease when the Skull Length goes up by 1? Here, we expect for every unit change in skull length, skull width *increases* by 1.0663. Remember in this case, this value is the slope!  We can also write this as $1.0663\pm0.1557$ to show the standard error!!
    + **Coefficient significance**: The P-value for any numeric intercept is derived from a standard null hypotheis: "The coefficient = 0." Significant P-values can be interpreted to mean that it is unlikely the true coefficient is 0 (but caution, this is NOT exactly what P-values mean). The P-value here is highly significant ($P=2.45e-10$), so we are reasonably confident that the true value indeed differs from 0.
+ **Residual standard error**: The standard error associated with residuals. Don't worry much about this line for thsi class.
+ **Multiple R-squared** and	**Adjusted R-squared** give the **R^2** associated with the model!! This is SUPER important and *it is actually the squared Pearon correlation coefficient*. The "adjusted R^2" is somewhat more reliable for consideration - it corrects for any overfitting artifacts inherent in modeling, so focus on this value. **A model's R^2 tells you the percent of variation in the response that can be explained by the predictors**. It's associated P-value is on the next line (here, `p-value: 2.446e-10`), and the associated standard null for this P-value is "R^2 = 0."
  + **In this model, 25.388% of the variation in skull widths can be explained by skull lengths. 74.62% of the varation in skull widths is therefore UNEXPLAINED by our model.** That 74.62% is sneakily hidden in the $\epsilon$ term of the linear model equation. 
  
  
**From this model, we have discovered the following:**

+ ~25.3% of variation in sparrow skull widths can be explained by skull lengths. While highly significant, this is a relatively low $R^2$ (perhaps we would have benefitted by including more information in our model!). Remember: effect size is NOT the same as significance!
+ For every unit increase in skull length, we expect weight to increase, on average, by ~1.0663.
+ We did not reject the null that the intercept is 0, so we expect expect the average sparrow with a skull length of 0 to have a weight of 0. But we also recognize that this is ridiculous and just because the math says it doesn't mean it's biologically meaningful.

#### Visualizing the results

We can visualize this output with a scatterplot and `geom_smooth()`! This is why we've been saying `method = "lm"` in `geom_smooth()` - this function adds a trendline, and by specify "lm" we are telling `geom_smooth()` to use a linear model to determine the trendline:
```{r, fig.width = 6, fig.height = 4}
# predictor goes on X, response goes on Y
ggplot(sparrows, aes(x = Skull_Length, y = Weight)) + 
  geom_point() + 
  geom_smooth(method = "lm", 
              color = "blue",     ## make the trendline blue
              fill = "goldenrod") + ## make the confidence interval** goldenrod for fun
  labs(x = "Skull Length", y = "Weight", title = "Linear model to predict sparrow weight") + 
  annotate("text",                ## geom to annotate with is text annotation
           x = 30, y = 30,         ## coordinates of the label, decided after trying several places..
           label = "R^2 == 0.253", ## label itself, double equals is needed for use with parse=T
           parse=T,               ## makes label actually show as a formula, with squared as superscript!
           color = "firebrick", size = 5)   ## font color and size 
```


**Critical followup: What is a confidence interval?** A confidence interval (CI) is meant to help convey *error* in the estimate - in this case, the confidence bands you see (yellow area around the line) represents the error associated with our fitted slope aka our model's $\beta_1$ estimate! Loosely speaking, a *95% CI* (ggplot and many others use 95% confidence interval by default) means: Assuming all the lovely assumptions of our statistical framework, there is 95% probability that the TRUE VALUE OF THE SLOPE is within the CI. Stated more accurately in context: If you took X random samples of sparrows and calculated their regressions of weight across skull length, 95% of the time the line (slope and intercept!) would fall within the 95% shaded area CI.

**For an excellent intuitive understanding of what this means, see [the second plot example in this package's README](https://github.com/wilkelab/ungeviz).**

#### Checking the residuals

One of the assumptions of linear models is that residuals should be normally distributed. Residuals are calculated as part of the model itself, so the goal is to check the residuals AFTER you perform the model to see if it worked out ok. If the residuals are severely not normal, it means there were some problems with the model itself and you need to rethink your approach - which predictors to include/exclude? add in interaction effect [keep reading!!]? transform some of the data?.

We can check this assumption using a **QQ plot**, specifically a NORMAL QQ plot which shows the relationship between your data and a *theoretical prediction* for what the data would like if it were normally distributed. If the data falls roughly in a straight line, the data is distributed normally!!! If not, not normal :( 

First, we need to get the residuals themselves out of the model - my preferred way for doing this uses the `broom()` package. This packages tidies (sweeps up?...sigh.) model output into nice little tibbles, and it has three main functions we want: `broom::tidy()`, `broom::glance()`, and `broom::augment()`. The first two are nice for collecting the ugly summary data into tibbles. We simply pass in the model output itself.

```{r}
# assumes broom library is loaded! 
# it comes installed with tidyverse, but needs to be loaded with library(broom) in the setup chunk.

# Gives coefficients in tidy form
broom::tidy(model_fit)

# Gives R^2 (and some other values, stay tuned!!!) in tidy form
broom::glance(model_fit)

## Gives a whoooole bunch of under-the-hood info in tidy form - tacked onto the data itself!!!
broom::augment(model_fit)
```


Focusing on the `augment()` output, we see there are 136 rows - indeed there are 136 sparrows!! Each row represents an outcome from the model. The particular columns we may want are `.fitted` and `.resid`:

+ `.fitted`: What does the model formula give for the response at this predictor? For example, row 1 is an observation with skull length of 31.2 and weight of 24.5. If I put the *predictor* 31.2 into the model $y = 1.0663x - 8.148$, I'd get: **1.0663*31.2 - 8.149 = 25.1**!! We won't use this quantity now, but it's really nice to know about.
+ `.resid`: The residual associated with that row - this column contains our residuals!!!

For making QQ plots, we'll use the base R functions `qqnorm()` (for a **norm**al distribution QQ plot) and `qqline()` (in this case base R is a lot easier than ggplot2). They are run as *separate commands entirely*, but R will always assume `qqline()` should go on top of the *most recently run QQ plot*:

```{r}
## plot
broom::augment(model_fit) -> augmented_fit

# plot the .resid column (yes it starts with a period), and add a line for visual guidance. 
qqnorm(augmented_fit$.resid)
qqline(augmented_fit$.resid, col = "red")  #Personal preference, I like making it a color. base R uses "col"
```

This is pretty good, but could be better - the tails of the plot has a bit of deviation, but not so much that I'd worry. So, I'm satisified with the assumptions being met for this model.

*Here are examples of how the plot might look when it's time to start worrying:*
```{r, echo=F, fig.height=4, fig.width=10}

par(mfrow=c(1,3))
x <- log(1:50)
qqnorm(x)
qqline(x)
x <- c(rnorm(100, 10, 1), rnorm(20, 5, 1), rnorm(20, 15, 1))
qqnorm(x)
qqline(x)
x <- (1:50)**3
qqnorm(x)
qqline(x)

```



### Simple ANOVA: Single categorical predictor

What if, rather than a numeric predictor (Skull Length), we had a *categorical* predictor, say Age? Here we might ask: **Does age predict weight in sparrows**? 

Again, begin with a quick-and-dirty visualization. Here we'd like to see that the distribution of skull widths has similar *spread* between ages (assumption of equal variance for categorical predictors!). Indeed, these two distributions show similar amounts of spread (how much relative space along the Y axis they take up), so assumption is met.

```{r, fig.width = 4, fig.height = 3}
ggplot(sparrows, aes(x = Age, y = Weight, color=Age)) + geom_jitter()
```


Let's run the model:
```{r}
## perform linear model and save as model_fit
model_fit <- lm(Weight ~ Age, data = sparrows)

## view output with summary(). 
summary(model_fit)
```

*This is where factors become really important*: All linear model output when categorical predictors are used assumes a given LEVEL of the categorical variable. Unless you re-factor a variable explicitly, levels will be used in alphabetical order - here, "Adult" comes before "Young".

We see the intercept is highly significant ($P<2e-16$) with a value of 25.49. For a model with a categorical predictor, this value means: *What is the expected weight for your average ADULT (the first level of Age) sparrow*? 

The coefficient here is associated with `AgeYoung` - it means, on average, we expect YOUNG birds to have weights on average 0.0765 units larger compared to Adult birds. Notably, for a categorical variable with $N$ levels, there will be see $N-1$ coefficients (one for each of the non-default). *Interpret categorical coefficients as the relative increase (or decrease if negative!) of response for this predictor level compared to the first/default predictor level.* The default here is Adult (alphabet!!). In this case, however, the P-value is NOT significant (P=0.772). Similarly our adjusted $R^2$ is not significant.

**This model therefore tells us: Age is NOT predictive of weight in sparrows!** Either way, let's think how we might visualize the results. The strip plot used above to check assumptions is a pretty good way to do it, or any other way to show distributions. That said, these models operate on _means_, so it might be useful to actually visualize the means specifically. A neat trick for doing this comes from `stat_summary()`!!

```{r, fig.width = 6, fig.height = 4}
ggplot(sparrows, aes(x = Age, y = Weight, color = Age)) + 
  geom_jitter() + 
  stat_summary(geom = "point",  ## draw a point
               fun   = "mean",  ## FUNction to apply: take the mean
               color = "black", size = 4) ## point styling
```

As you can see, those means look about identical!! It makes sense that the model did *not* show evidence that Age is predictive of Weight.

### LM with numeric and categorical predictors

Let's see how to run and interpret a model with BOTH numeric and categorical predictors. We will examine how Age AND Skull Length might be predictive of Weight in sparrows. Importantly, when you have multiple predictors, the model assumes they are FULLY INDEPENDENT and ENTIRELY UNRELATED. Of course this is not always true, and the next section shows how to deal with that issue.

Let's run the model with multiple predictors, which we simply add together in the formula
```{r}
## perform linear model and save as model_fit
model_fit <- lm(Weight ~ Age + Skull_Length, data = sparrows)

## view output with summary(). 
summary(model_fit)
```

When considering BOTH Age and Skull Length, we find:

+ The intercept means your average Adult bird with a skull length of 0 has a weight of -8.26. It's insigificant and unrealistic - moving on.
+ The "AgeYoung" means *when controlling for skull length*, each Young bird will weigh ~0.118 more than an Adult bird. Again, it's insigificant so we actually have no evidence the weight increase between Adult/Young differs from 0.
+ The "Skull_Length" coefficient means *when controlling for age*, the average bird's weight increases by 1.07 for each unit increase in skull length, and it's highly significant! Something! 
+ The predictors explain $~24.9%$ of the variation in weight. **Critically, this is LESS THAN the model with Skull_Length as the only predictor** (before, $R^2=0.2538$). By including the Age predictor, we in fact added NOISE rather than INFORMATION to the model - so we are actually LESS predictive.
+ ~75% of variation in weight is unexplained by the significant predictor Skull Length.

**How can we visualize this model?** We can again make a scatterplot, and show the trend lines for each Age:

```{r, fig.width = 6, fig.height = 4}
ggplot(sparrows, aes(x = Skull_Length, y = Weight, color = Age)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Skull Length") + 
  annotate("text", x = 30.5, y = 30, label = "R^2 == 0.249", parse=T, size=5)
```

This visualization shows us what the model did in numbers: See how the confidence bands for Adult and Young are completely overlapping???? This gives us a strong sense that Age does NOT have an effect on weight here - otherwise, the two trend lines would be fully distinct. **All that said, when we find that certain predictors are NOT significant, it is usually NOT A GOOD IDEA to keep them in the model because they capture noise rather than real signal.** We'll learn more about this later: How do you choose which predictors are helpful to include, and which are helpful to exclude, to get the most predictive power?*


### LM with multiple numeric predictors

Now we will look at a model with multiple numeric predictors, Skull Width *and* Skull Length. We know already that skull length has a linear relationship with weight from earlier in this document, so let's just check out the linearity of weight/skull width. Indeed, the relationship is linear, so let's proceed.

```{r, fig.width = 3, fig.height = 3}
ggplot(sparrows, aes(x = Skull_Width, y = Weight)) + geom_point()
```

Again, we can simply add predictors together (order does not matter!):
```{r}
## perform linear model and save as model_fit
model_fit <- lm(Weight ~ Skull_Width + Skull_Length, data = sparrows)

## view output with summary(). 
summary(model_fit)
```

Our model has found:

+ You average bird with skull width AND skull length of zero will have, on average, a weight of -14.6, and this is highly significant at $P=0.00557$. But of course, this is not biologically meaningful - it just is needed to build the model.
+ On average, we expect that weight increases by 1.0524 when skull width increases by a unit of 1. This is also highly significant at $P=0.00188$.
+ On average, we expect that weight increases by 0.76 when skull length increases by a unit of 1. This is also highly significant at $P=4.05e-05$.
+ The independent effects of skull length and width explain roughly 30.1% of variation in weight, and it is highly significant at $P=1.678e-11$. 

We could visualize this if we want by putting on predictor on the X, and using color to disinguish the other predictor. That said, it is very difficult to make scatterplot visualizations with multiple numeric predictors, especially since we can't draw two trend lines in one set of axes.

```{r, fig.width = 6, fig.height = 4}
ggplot(sparrows, aes(x = Skull_Length, y = Weight, color = Skull_Width)) + 
  geom_point(size = 2.5) + # i like the plot with bigger points 
  labs(x = "Skull Length", color = "Skull Width") +
  scale_color_distiller(palette = "Reds") +
  annotate("text", x = 30.5, y = 30, label = "R^2 == 0.301", parse=T, size=5) + 
  theme(legend.position = "bottom") # again my personal preference
```



### LM with interaction effects

<!--

 summary(lm(Weight ~ Femur_Length * Skull_Length, data = sparrows))

 summary(lm(Weight ~ Femur_Length * Skull_Length, data = sparrows))

The problem with the last two analyses was that they ASSUMED the predictors were fully independent of one another - this assumption of independent predictors is in fact a key assumption of linear models. But of course, it is not always the case that predictors are completely independent - Age and Skull Length, and almost certainly Skull Length and Skull Width, themselves are probably related to a degree! We can directly test and/or control for this non-independence using **interaction effects**. 

The rule is **significant interacton effects trump significant independent effects.**

--> 
